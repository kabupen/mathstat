\documentclass[10pt, a4paper]{ltjsarticle}

\usepackage{../../mathstat}

\begin{document}

\section{多項式曲線フィッテイング}

回帰問題では観測値 $(\bm{x}, \bm{t})$ の組み合わせを用いて、未知の入力変数の値に対して目的変数の値を
予測する問題になる。有限事のデータ集合から汎化しなければならない。
データに対する予測値を $y(x,w)$ とすると、線形モデル\footnote{データ$x$に対しては非線形モデルであるが、パラメータ$w$については線形モデルである}では
\begin{equation}
  y(x, \bm{w}) = w_0 + w_1x + w_2x^2 + ... = \sum w_jx^j
\end{equation}
予測値$y$と目的変数$t$（どちらも観測値の情報）のズレを誤差関数として表し
\begin{equation}
  E(w) = \frac{1}{2}\sum \{y(x_n, w) - t_n\} ^2
\end{equation}
この誤差が最小となるようにパラメータ $w$ を選ぶことで、回帰問題を解く。


\section{多項式曲線フィッテイング（その2）}

曲線フィッテイングでは、実際には観測値$x$に対して目的変数 $t$ はノイズが乗っていると考えられ、不確実性がある。
この不確実性を表現するために確率分布を用いることができる。入力情報 $x$ に対して、対応する$t$ は平均が $y(x,w)$、
分散が $\beta^{-1}$ で与えられるガウス分布に従うとすると
\begin{equation}
  p(t|x,w,\beta) = N(t|y(x,w), \beta^{-1})
\end{equation}
となる。
こうすることで、訓練データ $(\bm{x},\bm{t})$ を用いることで、最尤法からパラメータ $w,\beta$ を求めることができる。
尤度関数は
\begin{equation}
  L = \prod_{i=1}^N N(t_i|y(x_i,\bm{w}), \beta^{-1})
\end{equation}
対数尤度関数は
\begin{eqnarray}
  \ln L &=& \sum_{i=1}^N \ln \frac{1}{\sqrt{2\pi}\sigma} \exp \left(\frac{(y(x_i,w)-t_i)^2}{2\sigma^2}\right) \\
  &=& \frac{-N}{2} \ln 2\pi + N \ln \sigma - \frac{1}{2\sigma^2} \sum \{(y(x_i,w)-t_i)\}^2
\end{eqnarray}
となる。尤度関数の最大化は、パラメータ $w$ を決定するという観点からは第三項目の二乗和誤差を最小化するという
ことと等価である。したがって、二乗和誤差関数は、ノイズがガウス分布に従うという仮定のもとで尤度の最大化の結果として
解釈することができる。

\end{document}
